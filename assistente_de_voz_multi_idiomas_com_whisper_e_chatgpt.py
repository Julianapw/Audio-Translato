# -*- coding: utf-8 -*-
"""Assistente de Voz Multi-Idiomas Com Whisper e ChatGPT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rHGq5N-sbEGtZsNUiQFT8q60BhRbj99b
"""

LANG_INPUT = 'pt'         # idioma de entrada (você fala em PT)
CHUNK_SEC  = 4            # duração de cada chunk (4–5s ajuda)
N_CHUNKS   = 6            # número de chunks na sessão
MODEL_SIZE = 'base'       # whisper: 'tiny'|'base'|'small'|'medium'|'large'
TARGET_LANGS = ['en', 'fr', 'de']  # idiomas de saída (tradução + voz)


# 1) Login Hugging Face (opcional, mas recomendado para download estável)

!pip install -q huggingface_hub
from huggingface_hub import login

HF_TOKEN = "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"  # Cole aqui seu token "hf_..." (ou deixe vazio para seguir sem login)
if HF_TOKEN:
    login(HF_TOKEN)
    print("Login Hugging Face OK.")
else:
    print("Sem HF_TOKEN. Tentando acesso público (pode falhar sob rate limit).")

RECORD = r"""
const sleep  = time => new Promise(resolve => setTimeout(resolve, time));

const b2text = blob => new Promise(resolve => {
  const reader = new FileReader();
  reader.onloadend = e => resolve(e.srcElement.result);
  reader.readAsDataURL(blob);
});

async function record(ms) {
  // Solicita permissão de microfone
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const recorder = new MediaRecorder(stream);
  const chunks = [];

  recorder.ondataavailable = e => chunks.push(e.data);
  recorder.start();

  await sleep(ms);

  return new Promise(resolve => {
    recorder.onstop = async () => {
      const blob = new Blob(chunks, { type: 'audio/webm' });
      const text = await b2text(blob); // data:audio/webm;base64,...
      // Para limpar os recursos do mic
      stream.getTracks().forEach(t => t.stop());
      resolve(text);
    };
    recorder.stop();
  });
}
"""

def record(sec=3):
  """
  Grava 'sec' segundos de áudio no navegador e salva como .webm no Colab.
  Retorna o caminho do arquivo .webm.
  """
  # Injeta o JS no notebook
  display(Javascript(RECORD))
  # Executa a função record(ms) do JS e recebe o base64
  js_result = output.eval_js(f'record({int(sec*1000)})')
  if not js_result or ',' not in js_result:
      raise RuntimeError("Falha ao capturar áudio no navegador. Verifique permissões do microfone.")

  # Decodifica e salva .webm
  audio_b64 = js_result.split(',')[1]
  audio_bytes = b64decode(audio_b64)
  webm_path = f"/content/request_audio_{int(time.time()*1000)}.webm"
  with open(webm_path, 'wb') as f:
      f.write(audio_bytes)
  return webm_path

# 2) Instala o ffmpeg para conversão .webm -> .wav
!apt-get -y install ffmpeg > /dev/null

def ensure_wav(in_path):
  """
  Converte um arquivo .webm (ou outro formato) para .wav (mono, 16 kHz).
  Retorna o caminho do .wav gerado.
  """
  if not os.path.exists(in_path):
      raise FileNotFoundError(f"Arquivo não encontrado: {in_path}")

  out_path = f"/content/request_audio_{int(time.time()*1000)}.wav"
  cmd = [
      "ffmpeg", "-y", "-i", in_path,  # entrada
      "-ac", "1",                      # 1 canal (mono)
      "-ar", "16000",                  # 16 kHz
      "-sample_fmt", "s16",            # PCM 16-bit
      out_path
  ]
  proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  if proc.returncode != 0:
      raise RuntimeError(f"ffmpeg falhou ao converter para WAV.\nSTDERR:\n{proc.stderr.decode('utf-8', 'ignore')}")
  return out_path

def record_and_prepare(sec):
  """
  Grava por 'sec' segundos, mostra preview, informa tamanho do arquivo,
  converte para WAV e retorna (wav_path, webm_path, file_size_bytes).
  """
  # Grava .webm
  webm_path = record(sec)
  size = os.path.getsize(webm_path)
  print(f"↳ Arquivo gravado: {webm_path} | tamanho: {size} bytes")

  # Preview do .webm
  try:
      display(Audio(webm_path, autoplay=False))
  except Exception as e:
      print("Aviso: não foi possível reproduzir o .webm no preview. Erro:", e)

  # Se muito pequeno, provavelmente silêncio ou permissão negada
  if size < 5000:  # ~5 KB
      print("⚠️ Áudio muito curto/silencioso. Verifique microfone e fale durante todo o chunk.")
      return None, webm_path, size

  # Converte para .wav (mais compatível com whisper)
  wav_path = ensure_wav(webm_path)

  # Preview do .wav (opcional)
  try:
      display(Audio(wav_path, autoplay=False))
  except Exception as e:
      print("Aviso: não foi possível reproduzir o .wav no preview. Erro:", e)

  return wav_path, webm_path, size

# 3) Teste rápido (usa CHUNK_SEC da sua célula de Configurações)
try:
    _ = CHUNK_SEC
except NameError:
    CHUNK_SEC = 3  # caso não tenha definido antes

print(f"Teste de gravação de {CHUNK_SEC}s...")
wav_path, webm_path, size = record_and_prepare(CHUNK_SEC)
if wav_path is None:
    print("Sem áudio útil no teste. Ajuste permissões/CHUNK_SEC e tente novamente.")
else:
    print("OK: áudio capturado e convertido para WAV:", wav_path)


# 3) Whisper — transcrição em PT

!pip install -q git+https://github.com/openai/whisper.git

import whisper
whisper_model = whisper.load_model(MODEL_SIZE)

def transcribe_audio(file_path, lang_in=LANG_INPUT):
    """Transcreve o áudio (WAV/WebM) em PT."""
    res = whisper_model.transcribe(file_path, fp16=False, language=lang_in)
    return res.get("text", "").strip()


# 4) Tradução multilíngue (M2M100)

!pip install -q transformers sentencepiece

import torch
from transformers import pipeline

# Se você fez login, use o mesmo HF_TOKEN; caso contrário, segue sem token.
token_arg = {"token": HF_TOKEN} if 'HF_TOKEN' in globals() and HF_TOKEN else {}

tr_pt_en = pipeline(
    "translation",
    model="facebook/m2m100_418M",
    src_lang="pt", tgt_lang="en",
    device=0 if torch.cuda.is_available() else -1,
    **token_arg
)
tr_pt_fr = pipeline(
    "translation",
    model="facebook/m2m100_418M",
    src_lang="pt", tgt_lang="fr",
    device=0 if torch.cuda.is_available() else -1,
    **token_arg
)
tr_pt_de = pipeline(
    "translation",
    model="facebook/m2m100_418M",
    src_lang="pt", tgt_lang="de",
    device=0 if torch.cuda.is_available() else -1,
    **token_arg
)

def translate_all_pt(text_pt: str):
    """Retorna {'en':..., 'fr':..., 'de':...} a partir de PT."""
    if not text_pt or not text_pt.strip():
        return {'en':'', 'fr':'', 'de':''}
    out_en = tr_pt_en(text_pt)[0]['translation_text']
    out_fr = tr_pt_fr(text_pt)[0]['translation_text']
    out_de = tr_pt_de(text_pt)[0]['translation_text']
    return {'en': out_en, 'fr': out_fr, 'de': out_de}


# 5) Síntese de voz (gTTS)

!pip install -q gTTS
from gtts import gTTS
from IPython.display import Audio, display

SUPPORTED_GTTs = {'en','fr','de','pt'}

def speak(text, lang='en'):
    if not text or not text.strip():
        return None
    if lang not in SUPPORTED_GTTs:
        raise ValueError(f"Idioma {lang} não suportado pelo gTTS! Suportados: {SUPPORTED_GTTs}")
    import time
    out_file = f"/content/response_{lang}_{int(time.time()*1000)}.mp3"
    gTTS(text=text, lang=lang, slow=False).save(out_file)
    display(Audio(out_file, autoplay=True))
    return out_file


# 6) Loop principal

print(f"\n=== Tradução com M2M100 | Idiomas: {TARGET_LANGS} | Chunks: {N_CHUNKS} × {CHUNK_SEC}s ===")
print("Fale algo em português e comece a falar imediatamente em cada chunk.")

transcript_full, outputs_full = [], []
import os as _os

for i in range(N_CHUNKS):
    print(f"\n[Gravação {i+1}/{N_CHUNKS}] Ouvindo...")
    wav_path, webm_path, size = record_and_prepare(CHUNK_SEC)

    if wav_path is None:
        transcript_full.append("")
        continue

    # 1) Transcrever (PT)
    text_in = transcribe_audio(wav_path, lang_in=LANG_INPUT)
    print(f"↳ Você ({LANG_INPUT}): {text_in}")
    transcript_full.append(text_in)

    if not text_in or not text_in.strip():
        print("⚠️ Transcrição vazia. Tente falar desde o início do chunk.")
        continue

    # 2) Traduzir PT -> EN/FR/DE
    translations = translate_all_pt(text_in)

    # 3) Falar cada idioma
    for lang_code in TARGET_LANGS:
        out_text = translations.get(lang_code, "")
        print(f"↳ [{lang_code}] {out_text}")
        outputs_full.append((lang_code, out_text))
        speak(out_text, lang=lang_code)

print("\n=== Sessão encerrada ===")
print("\nResumo (últimos trechos):")
for t in transcript_full[-5:]:
    print("• (IN/PT)", t)
for lang_code, out_text in outputs_full[-5:]:
    print(f"• (OUT/{lang_code})", out_text)
